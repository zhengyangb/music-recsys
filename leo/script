JAVA_OPTS="-Xmx512000000 "
export JAVA_OPTS
PYSPARK_PYTHON=$(which python) pyspark


from pyspark.sql import SparkSession
from pyspark import SparkConf
from pyspark.ml.feature import StringIndexer
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.mllib.evaluation import RankingMetrics
import pyspark.sql.functions as F
from pyspark.sql.functions import expr
import itertools as it
##############################################
train_path = 'hdfs:/user/bm106/pub/project/cf_train.parquet'
val_path = 'hdfs:/user/bm106/pub/project/cf_validation.parquet'
train = spark.read.parquet(train_path)
val = spark.read.parquet(val_path)

indexer_user = StringIndexer(inputCol="user_id", outputCol="user_id_indexed")
indexer_user_model = indexer_user.fit(train)
indexer_track = StringIndexer(inputCol="track_id", outputCol="track_id_indexed", handleInvalid='skip')
indexer_track_model = indexer_track.fit(train)

train = indexer_user_model.transform(train)
train = indexer_track_model.transform(train)

val = indexer_user_model.transform(val)
val = indexer_track_model.transform(val)

i = [5,0.1,10]
als = ALS(rank = i[0], maxIter=5, regParam=i[1], userCol="user_id_indexed", itemCol="track_id_indexed", ratingCol="count", implicitPrefs=True, \
            alpha=i[2], nonnegative=True, coldStartStrategy="drop")
model = als.fit(train)

user_id = val.select('user_id_indexed').distinct()
res = model.recommendForUserSubset(user_id,500)
pred_label = res.select('user_id_indexed','recommendations.track_id_indexed')

# val.createOrReplaceTempView('val')
# user_id = spark.sql('SELECT DISTINCT(user_id_indexed) FROM val')
# predictions = model.transform(val)
# pred_label = predictions.select('user_id_indexed', 'track_id_indexed', 'prediction')\
#                         .groupBy('user_id_indexed')\
#                         .agg(expr('collect_list(track_id_indexed) as pred_item'))

true_label = val.select('user_id_indexed', 'track_id_indexed')\
                        .groupBy('user_id_indexed')\
                        .agg(expr('collect_list(track_id_indexed) as true_item'))

pred_true_rdd = pred_label.join(F.broadcast(true_label), 'user_id_indexed', 'inner') \
            .rdd \
            .map(lambda row: (row[1], row[2]))

metrics = RankingMetrics(pred_true_rdd)
ndcg = metrics.ndcgAt(500)
mpa = metrics.precisionAt(500)
print(i, ndcg, mpa)
##############################################


train = spark.read.parquet('hdfs:/user/zb612/transformed_train.parquet')
# ALS model
als = ALS(maxIter=5, regParam=0.01, userCol="user_id_indexed", itemCol="track_id_indexed", ratingCol="count", implicitPrefs=True, alpha=1.0, nonnegative=True, \
	coldStartStrategy="drop")
model = als.fit(train)

val = spark.read.parquet('hdfs:/user/bm106/pub/project/cf_validation.parquet')
val = indexer_user.fit(val).transform(val)
val = indexer_track.fit(val).transform(val)
predictions = model.transform(val)

pred_label = predictions.select('user_id_indexed', 'track_id_indexed', 'prediction')\
						.groupBy('user_id_indexed')\
						.agg(expr('collect_list(track_id_indexed) as pred_item'))

true_label = val.select('user_id_indexed', 'track_id_indexed')\
						.groupBy('user_id_indexed')\
						.agg(expr('collect_list(track_id_indexed) as true_item'))

pred_true_rdd = pred_label.join(F.broadcast(true_label), 'user_id_indexed', 'inner') \
            .rdd \
            .map(lambda row: (row[1], row[2]))

metrics = RankingMetrics(pred_true_rdd)
ndcg = metrics.ndcgAt(500)
mpa = metrics.precisionAt(500)


## ALS.trainImplicit is the old version
## use implicitPrefs = True for the new version
#finding best set of parameters
ranks  = [5,10,15,20]
reguls = [0.1, 1,10]
alpha = [10, 20, 40]

finalModel = None
finalRank  = 0
finalRegul = float(0)
finalIter  = -1
finalDist   = float(300)
finalAlpha = float(0)

#[START train_model]
for cRank, cRegul, cIter, cAlpha in itertools.product(ranks, reguls, iters, alpha):
    model = ALS.trainImplicit(rddTraining, cRank, cIter, float(cRegul),alpha=float(cAlpha))
    dist = howFarAreWe(model, rddValidating, nbValidating)
    if dist < finalDist:
        print(cIter, cRank,cAlpha,cRegul)
        print("Best so far:%f" % dist)
        finalModel = model
        finalRank  = cRank
        finalRegul = cRegul
        finalIter  = cIter
        finalDist  = dist
        finalAlpha  = cAlpha

print("Rank %i" % finalRank)
print("Regul %f" % finalRegul)
print("Iter %i" % finalIter)
print("Dist %f" % finalDist)
print("Alpha %f" % finalAlpha)

model = ALS.trainImplicit(rddTraining, rank=finalRank, iterations=finalIter, lambda_= float(finalRegul),alpha=float(finalAlpha))
predictions = model.transform(test)

## make predictions
row = [3704.0]
myrdd = sc.parallelize([row])
schemaPeople = sqlContext.createDataFrame(myrdd, schema)
res = model.recommendForUserSubset(schemaPeople,5)
res.show()

val.createOrReplaceTempView('val')
user_id = spark.sql('SELECT DISTINCT(user_id_indexed) FROM val')
res = model.recommendForUserSubset(user_id,2)

full = user_id.crossJoin(item_id)
full = full.selectExpr("user_id_indexed as user_id_indexed","id as track_id_indexed")
